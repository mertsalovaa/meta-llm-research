# Дослідження вендора LLM: Meta

> **Виконала:**  
> студентка групи ІПЗ-51м  
> Мерцалова Ірина  

## Зміст
1. [Вступ: Meta як архітектор відкритої екосистеми ШІ](#1-вступ)
2. [Еволюція моделей Meta LLM](#2-еволюція-моделей-meta-llm)
    * [OPT](#21-opt)
    * [Galactica](#22-galactica)
    * [Llama 1](#23-Llama-1)
    * [Llama 2](#24-Llama-2) 
    * [Code Llama](#25-code-llama)
    * [Llama 3](#26-llama-3) 
    * [Llama 3.1](#27-llama-31) 
    * [Llama 3.2](#28-llama-32) 
    * [Llama 3.3](#29-llama-33) 
    * [Llama 4](#210-llama-4) 
    * [Llama Guard](#211-llama-guard) 
3. [Карта моделей](#3-карта-моделей)
4. [Ключові персони](#4-ключові-персони)
5. [Бенчмарки та порівняння](#5-бенчмарки-та-порівняння-ефективності)
6. [Інструменти та підходи Meta для розробки моделей Llama](#6-інструменти-та-підходи-meta-для-розробки-моделей-llama)
    * [Репозиторії, відкритий код та "Llama Stack"](#61-репозиторії-відкритий-код-та-llama-stack)
    * [Доступність моделей, ліцензування і open-weights політика](#62-доступність-моделей-ліцензування-і-open-weights-політика)
    * [Оптимізації та інструменти для inference/fine-tuning](#63-оптимізації-та-інструменти-для-inferencefine-tuning)
    * [Спеціалізовані моделі: Code, Safety, Multimodal/Екосистема](#64-спеціалізовані-моделі-code-safety-multimodalекосистема)
    * [Висновок](#65-висновок)
7. [Джерела](#джерела)


## 1. Вступ: Meta як архітектор відкритої екосистеми ШІ

Компанія Meta (раніше Facebook) займає унікальну та визначальну позицію в сучасному 
світі генеративного штучного інтелекту. На відміну від своїх головних конкурентів – OpenAI (Microsoft) та Google, які обрали шлях закритих комерційних систем (closed source), Meta відіграла ключову роль у відкритті доступу до LLM (**Large Language Models** – великі мовні моделі).

Головна особливість Meta – це стратегія Open Weights (відкритих ваг) – це означає, що компанія віддає всі знання, які вона отримала за роки навчання своїх моделей, у відкритий доступ. Саме це дозволяє будь-якому розробнику завантажити модель собі на комп'ютер, користуватися нею без інтернету і, що найважливіше, донавчати її під свої власні, специфічні завдання.

*Ця стратегія має дві практичні цілі:*
1.  **Стандартизація індустрії –** Meta прагне, щоб більшість розробників використовували саме архітектуру Llama, що дозволяє індустрії створювати нові інструменти та програми саме під Meta, закріплюючи за ними лідерство на ринку.
2.  **Краудсорсинг інновацій –** відкриваючи доступ до моделей, компанія залучає світову спільноту до роботи. Розробники знаходять помилки, оптимізують код та створюють нові методи використання швидше, ніж це могла б зробити будь-яка внутрішня команда.

Варто також зазначити масштаб інвестицій в інфраструктуру. За даними Reuters, Meta прогнозує капітальні витрати на рівні до $65 млрд у 2025 році [[1]](#джерела). Як зазначив CEO Марк Цукерберг, ці кошти спрямовані на розширення AI-інфраструктури, щоб посилити позиції компанії у гонці проти OpenAI та Google.

Окрім масової закупівлі чіпів NVIDIA, Meta робить важливий стратегічний крок – компанія почала тестування власних процесорів (in-house chips) для тренування ШІ [[2]](#джерела). Це дозволить зменшити залежність від зовнішніх постачальників (зокрема NVIDIA) та отримати більш енергоефективні рішення для специфічних завдань.

Таким чином, Meta виступає не просто як вендор моделей, а як творець інфраструктурної платформи, на якій будується значна частина сучасної open-source AI економіки.


## 2. Еволюція моделей Meta LLM

На початку десятиліття штучний інтелект став ключовою технологічною сферою конкуренції між провідними компаніями світу. У 2021–2022 роках OpenAI, Google та інші лідери технологій активно розвивали LLM, але більшість з них залишалися закритими та недоступними для розробників. Саме тому, Meta обрала іншу стратегію – масштабна відкритість, підтримка академічної спільноти та розвиток *open-source*-екосистеми.

Метою Meta було створення сімейства моделей, які можуть бути використані у наукових дослідженнях, промислових застосуваннях та освітніх проєктах без бар’єрів комерційної ліцензії – цей підхід став фундаментом усієї подальшої еволюції Meta LLM.

### 2.1. OPT

**OPT (Open Pre-trained Transformer)** став першою масштабною відповіддю Meta на обмеженість доступу до LLM. Модель мала варіації від 125M до 175B параметрів та відтворювала архітектуру GPT-3, але у відкритому форматі. OPT став відправною точкою, що відкрила Meta шлях до більш масштабних моделей.

Особливості:
* відтворення GPT-3-класи моделей у відкритому доступі
* публікація повного навчального коду та журналів тренування
* орієнтація на дослідників та університети

### 2.2. Galactica

**Galactica** була експериментальною моделлю, натренованою на наукових публікаціях, підручниках, хімічних та біологічних базах знань, та мала варіації від 125M до 120B параметрів.

Ключові характеристики:
* 120B параметрів у флагманській версії
* домен – наукова інформація
* генерація формул, цитувань, наукових пояснень

Попри високий рівень інноваційності, модель була знята з публічного доступу через ризик генерації неправдивого контенту. Проте досвід Galactica інтегрувався в наступні покоління LLM.

### 2.3. Llama 1

У 2023 році Meta представила Llama (Large Language Model Meta AI) – нову архітектуру, оптимізовану для інференсу на менш потужному обладнанні. Llama 65B та Llama 33B навчені на 1,4 трильйона токенів. Найменша модель, Llama 7B, навчена на одному трильйоні токенів.

Нововведення Llama 1:
* висока якість при менших параметрах
* відкритість моделі та можливість донавчання спільнотою
* зростання ролі open-source у LLM-ринку

Фактично, саме Llama 1 зробила Meta глобальним лідером у сфері open-sourse моделей.

### 2.4. Llama 2

Llama 2 стала важливим кроком уперед – вперше Meta дозволила комерційне використання моделей. Вона складається з попередньо навчених та точно налаштованих генеративних текстових моделей масштабом від 7 до 70 мільярдів параметрів.

Точно налаштовані LLM, які називаються Llama-2-Chat, оптимізовані для використання в діалогах. Моделі Llama-2-Chat перевершують моделі чату з відкритим кодом у більшості протестованих нами бенчмарків, а в наших людських оцінках корисності та безпеки вони знаходяться на одному рівні з деякими популярними моделями із закритим кодом, такими як ChatGPT та PaLM. [[3]](#джерела)

Покращення порівняно з Llama 1:
* збільшений обсяг тренувальних даних
* контрольоване точне налаштування (SFT)
* навчання з підкріпленням та людським зворотним зв'язком (RLHF)
* значно краща якість у завданнях міркування
* спеціальна модель Code Llama для програмування

### 2.5. Code Llama

**Code Llama** – перша велика модель від Meta, створена для генерації та аналізу коду, що забезпечує найсучаснішу продуктивність серед відкритих моделей, можливості заповнення, підтримку великих вхідних контекстів та можливість виконання інструкцій без повторного виконання для програмних завдань. 

Для навчання моделі використовувались спеціальні бібліотеки, тому точне налаштування та навчання були виконані за допомогою дослідницького суперкластеру Meta.

Саме Code Llama стала одним із найпопулярніших інструментів серед розробників у 2023-2024.

Переваги:
* підтримка понад 20 мов програмування
* здатність розуміти великі фрагменти коду, рефакторинг, коментування.
* варіації Code Llama випускається в чотирьох розмірах моделі та трьох варіантах: 
    * *Code Llama*: базова модель, яка розробленв для загального синтезу та розуміння коду
    * *Code Llama - Python* розроблена спеціально для роботи з мовою програмування Python
    * *Code Llama - Instruct* призначена для безпечнішого використання в програмах допомоги та генерації коду [[4]](#джерела).

### 2.6. Llama 3

Meta розробила та випустила сімейство LLM Meta **Llama 3** – колекцію попередньо навчених та налаштованих на інструкції генеративних текстових моделей. Llama 3 випускається у двох розмірах – з параметрами 8B та 70B – у попередньо навчених та налаштованих за допомогою інструкцій варіантах, які оптимізовані для випадків використання діалогу та перевершують багато доступних моделей чату з відкритим кодом за загальними галузевими тестами. Крім того, розробляючи ці моделі, приділили значну увагу оптимізації корисності та безпеки.

Для попереднього навчання використовувались користувацькі навчальні бібліотеки, Meta's Research SuperCluster та виробничі кластери. Точне налаштування, анотування та оцінювання також виконувалися на сторонніх хмарних обчисленнях.

Llama 3 було попередньо навчено на понад 15 трильйонах токенів даних із загальнодоступних джерел. Дані для точного налаштування включають загальнодоступні набори даних інструкцій, а також понад 10 мільйонів прикладів з анотаціями від людини. Ні набори даних для попереднього навчання, ні набори даних для точного налаштування не містять метаданих користувачів.

Основні цінності LLM Llama 3 – це відкритість, інклюзивність та готовність допомогти. Вона призначена для обслуговування всіх та для широкого кола випадків використання, щоб бути доступним для людей з різним походженням, досвідом та поглядами. Llama 3 звертається до користувачів та їхніх потреб такими, якими вони є, без зайвих осудів чи нормативності, водночас відображаючи розуміння того, що навіть контент, який може здаватися проблематичним в одних випадках, може служити цінним цілям в інших. Вона поважає гідність та автономію всіх користувачів, особливо з точки зору цінностей свободи думки та слова, які сприяють інноваціям та прогресу [[5]](#джерела).

LLaMA 3 отримала найширшу підтримку в екосистемі open-source.

### 2.7. Llama 3.1

Колекція багатомовних моделей **Llama 3.1** – це колекція попередньо навчених та налаштованих на інструкції генеративних моделей для введення чи виведення текст, яка призначена для комерційного та дослідницького використання. Моделі Llama 3.1 оптимізовані для використання багатомовних діалогів та перевершують багато доступних моделей з відкритим кодом та закритим чатом за загальними галузевими тестами.

Вона була попередньо навчена на ~15 трильйонах токенів даних із загальнодоступних джерел. Дані для точного налаштування включають загальнодоступні набори даних інструкцій, а також понад 25 мільйонів синтетично згенерованих прикладів.

Llama 3.1 підтримує 7 мов на додаток до англійської: французьку, німецьку, хінді, італійську, португальську, іспанську та тайську. Llama може виводити текст іншими мовами, окрім тих, що відповідають пороговим значенням продуктивності для безпеки та корисності. Не рекомендується розробникам використовувати модель для спілкування непідтримуваними мовами без впровадження точного налаштування та системних елементів керування [[6]](#джерела).

Флагманська модель є конкурентоспроможною з провідними базовими моделями для виконання низки завдань, включаючи GPT-4, GPT-4o та Claude 3.5 Sonnet.


### 2.8. Llama 3.2

**Llama 3.2** – це реліз, який має мультимодальні версії (Vision), текстові lightweight-версії (1B, 3B) та доступність через комерційні/хмарні сервіси (AWS). 

Для підтримки завдань розпізнавання зображень модель Llama 3.2-Vision використовує окремо навчений адаптер зору, який інтегрується з попередньо навченою мовною моделлю Llama 3.1. Адаптер складається з серії шарів перехресної уваги, які передають представлення кодера зображень до основного LLM. Моделі з налаштованими інструкціями призначені для візуального розпізнавання, аналізу зображень, створення субтитрів та асистентоподібного чату із зображеннями.

Llama 3.2-Vision було попередньо навчено на 6B парах зображень та тексту. Дані налаштування інструкцій включають загальнодоступні набори даних інструкцій зору, а також понад 3M синтетично згенерованих прикладів [[7]](#джерела).

Ця модель перевершує багато доступних багатомодальних моделей з відкритим кодом та закритих багатомодальних моделей за загальними галузевими тестами.

### 2.9. Llama 3.3

**Модель Llama 3.3** – модель, яка працює лише з текстом, оптимізована для використання багатомовного діалогу та перевершує багато доступних моделей з відкритим кодом та закритим чатом за загальними галузевими тестами. Налаштовані версії використовують контрольоване точне налаштування (SFT) та навчання з підкріпленням та людським зворотним зв'язком (RLHF) для узгодження з людськими вподобаннями щодо корисності та безпеки. Llama 3.3 призначена для комерційного та дослідницького використання кількома мовами, а також підтримує можливість використання результатів своїх актуальних моделей для покращення інших моделей, включаючи генерацію та дистиляцію синтетичних даних [[8]](#джерела). 

За заявами Meta та партнерів, модель пропонує покращене «reasoning, coding, instruction-following» порівняно з earlier Llama версіями.

### 2.10. Llama 4

**Llama 4** – це колекція попередньо навчених та налаштованих на інструкції LLM зі змішаними експертами, що знаменують собою початок нової ери для екосистеми Llama. Запускається дві ефективні моделі серії Llama 4: *Llama 4 Scout* (модель з 17 мільярдами параметрів та 16 експертами) та *Llama 4 Maverick* (модель з 17 мільярдами параметрів та 128 експертами) – ці моделі оптимізовані для мультимодального розуміння, багатомовних завдань, кодування, виклику інструментів та підтримки агентних систем. Крайній термін знань для моделей – серпень 2024 року.

Llama 4 Scout було попередньо навчено на ~40 трильйонах токенів, а Llama 4 Maverick – на ~22 трильйонах токенів мультимодальних даних з поєднання загальнодоступних, ліцензованих даних та інформації з продуктів і сервісів Meta – це включає публічно поширені публікації з Instagram та Facebook, а також взаємодію людей з Meta AI.

*Llama 4* – це більш керована модель, тобто відповіді можна легко адаптувати до конкретних потреб розробника. Ефективні системні підказки можуть значно підвищити продуктивність великих мовних моделей. Зокрема, провелось тестування, що використання системної підказки може бути ефективним у зменшенні кількості помилкових відмов та шаблонних або «повічливих» мовних моделей, поширених у LLM. Вони також можуть покращити розмовність та використання відповідного форматування.

### 2.11. Llama Guard

Meta створила окрему родину моделей LLaMA Guard, що відповідає за:
* модерацію контенту;
* виявлення небезпечних запитів;
* фільтрацію небажаних відповідей;
* захист від небезпечних інструкцій.

Вони використовуються у Facebook, Instagram, WhatsApp, а також інтегруються в open-source-екосистему. Були представлені LLaMA Guard 1, 2, 3 та 4.


## 3. Карта моделей

Нижче наведено зведену таблицю ключових характеристик моделей Meta AI. Ця "карта" демонструє прогрес у масштабуванні параметрів, кількості розмірів моделей та типів.

### Порівняльна таблиця сімейства Meta LLM
---
|  **Модель** | **Розмір моделі** | **Тип** | **Дата випуску** |  **Довжина контексту** | **Токенізатор** | **Ліцензія** |
| :----: | :----: | :----: | :----: | :----: | :----: | :----: |
| OPT | 125M, 350M, 1.3B, 2.7B, 6.7B, 13B, 30B, 66B, 175B | Текст | 01.05.2022 | 2K | GPT-2 BPE | Некомерційні дослідження |
| Galactica | 125M, 1.3B, 6.7B, 30B, 120B | Текст | 01.11.2022 | 2K | GPT-2 BPE | CC BY-NC 4.0 |
| Llama (1) | 7B, 13B, 33B, 65B | Текст | 01.02.2023 | 2K | SentencePiece | Некомерційні дослідження |
| Llama 2 | 7B, 13B, 70B | Текст | 18.07.2023 |  4K | Sentencepiece | Llama 2 License (permissive) | 
| Llama 3 | 8B, 70B | Текст | 18.04.2024 |  8K | TikToken-based | Meta Llama 3 Community License | 
| Llama 3.1 | 8B, 70B, 405В | Багатомовний текст | 23.07.2024 |  128K | TikToken-based | Meta Llama 3.1 Community License |  
| Llama 3.2 | 1B, 3B | Багатомовний текст | 25.09.2024 |  128K | TikToken-based | Meta Llama 3.2 Community License | 
| Llama 3.2-Vision | 11B, 90B | Багатомовний текст + зображення | 25.09.2024 |  128K | TikToken-based | Meta Llama 3.2 Community License | 
| Llama 3.3 | 70B | Багатомовний текст | 06.12.2024 |  128K | TikToken-based | Meta Llama 3.3 Community License | 
| Llama 4 | Llama 4 Scout (17B) | Багатомовний текст + зображення | 05.04.2025 |  10М | TikToken-based | Meta Llama 4 Community License | 
| | Llama 4 Maverick (17B) | Багатомовний текст + зображення | 05.04.2025 |  1М | TikToken-based | Meta Llama 4 Community License | 
---

## 4. Ключові персони

Розвиток екосистеми Llama став можливим завдяки унікальній культурі підрозділу **FAIR (Fundamental AI Research)**, яка поєднує академічну свободу з ресурсами техногіганта. Успіх проєкту залежить від конкретних лідерів та інженерів.

---
| **Ім'я** | **Позиція** | **Роль** | **Вплив** | 
| :----: | :----: | :----: | :----: | 
| **Ян ЛеКун (Yann LeCun)** | Chief AI Scientist | Головний ідеолог штучного інтелекту в Meta, лауреат премії Тюрінга (аналог Нобелівської премії в IT) | Саме ЛеКун є головним захисником стратегії **Open Weights**. Він переконаний, що закритий ШІ (як у OpenAI) небезпечний, оскільки концентрує владу в одних руках. Його філософія – "ШІ має бути відкритою науковою дисципліною" |
| **Марк Цукерберг (Mark Zuckerberg)** | CEO Meta | Замовник та спонсор | Прийняв стратегічне рішення переорієнтувати компанію на AI First та виділив бюджет на закупівлю 350,000+ чіпів NVIDIA H100. Його підтримка open-source підходу дозволила зробити Llama безкоштовним стандартом індустрії |
| **Жоель Піно (Joelle Pineau)** | VP of AI Research | Керівниця лабораторій FAIR | Відповідає за баланс між фундаментальною наукою та створенням реальних продуктів. Під її керівництвом дослідницькі проєкти перетворилися на продуктову лінійку Llama |
| **Ахмад Аль-Дахле (Ahmad Al-Dahle)** | VP of Generative AI | Керівник продуктового підрозділу GenAI | Саме він відповідає за перетворення наукових досліджень FAIR на реальні продукти. Його команда займалася тренуванням та випуском Llama 3, фокусуючись на тому, щоб модель була корисною для кінцевого користувача (coding, reasoning) |
| **Уго Туврон (Hugo Touvron)** | Lead Author |  | Його ім'я стоїть першим у наукових статтях про Llama 1 та Llama 2. Він керував технічною розробкою архітектури та процесом тренування моделей |
| **Томас Сіалом (Thomas Scialom)** | Керівник напрямку Fine-tuning & RLHF |  | Саме він відповідав за те, щоб "сира" модель навчилася вести діалог, бути ввічливою та безпечною (Llama 2 Chat). Без його роботи модель була б просто генератором тексту, а не помічником |
---

### Кадрові переходи та "Феномен Mistral"

Meta стала "кузнею кадрів" для всієї індустрії. Найяскравішим прикладом є відтік талантів, що призвів до появи головного європейського конкурента – **Mistral AI**.

**Гійом Лампль (Guillaume Lample)** та **Тімоті Лакруа (Timothée Lacroix)**:
* *Історія:* Провідні дослідники Meta AI, які були співавторами статті про Llama 1.
* *Перехід:* У 2023 році вони покинули Meta і заснували французький стартап **Mistral AI**.
* *Наслідок:* Використовуючи досвід, отриманий у Meta, вони створили моделі (Mistral 7B, Mixtral 8x7B), які певний час перевершували Llama за ефективністю.

Цей "витік мізків" демонструє, що Meta збирає найсильніших інженерів світу, але іноді втрачає їх через бюрократію або бажання дослідників створювати власні продукти.

## 5. Бенчмарки та порівняння ефективності

### 1: Pre-trained Models (Базові моделі)

---
| Категорія | Бенчмарк | # Shots | Метрика | Llama 3.1 70B | Llama 3.1 405B | Llama 4 Scout | Llama 4 Maverick |
| :--- | :--- | :---: | :--- | :---: | :---: | :---: | :---: |
| **Reasoning & Knowledge** | MMLU | 5 | acc_char | 79.3 | 85.2 | 79.6 | 85.5 |
| | MMLU-Pro | 5 | macro_avg/em | 53.8 | 61.6 | 58.2 | 62.9 |
| | MATH | 4 | em_maj1@1 | 41.6 | 53.5 | 50.3 | 61.2 |
| **Code** | MBPP | 3 | pass@1 | 66.4 | 74.4 | 67.8 | 77.6 |
| **Multilingual** | TydiQA | 1 | average/f1 | 29.9 | 34.3 | 31.5 | 31.7 |
| **Image** | ChartQA | 0 | accuracy | *Не підтримується* | *Не підтримується* | 83.4 | 85.3 |
| | DocVQA | 0 | anls | *Не підтримується* | *Не підтримується* | 89.4 | 91.6 |

---

### 2: Instruction Tuned Models (Чат-версії)

---
| Категорія | Бенчмарк | # Shots | Метрика | Llama 3.3 70B | Llama 3.1 405B | Llama 4 Scout | Llama 4 Maverick |
| :--- | :--- | :---: | :--- | :---: | :---: | :---: | :---: |
| **Image Reasoning** | MMMU | 0 | accuracy | *Не підтримується* | *Не підтримується* | 69.4 | 73.4 |
| | MMMU Pro* | 0 | accuracy | *Не підтримується* | *Не підтримується* | 52.2 | 59.6 |
| | MathVista | 0 | accuracy | *Не підтримується* | *Не підтримується* | 70.7 | 73.7 |
| **Image Understanding** | ChartQA | 0 | accuracy | *Не підтримується* | *Не підтримується* | 88.8 | 90.0 |
| | DocVQA (test) | 0 | anls | *Не підтримується* | *Не підтримується* | 94.4 | 94.4 |
| **Code** | LiveCodeBench | 0 | pass@1 | 33.3 | 27.7 | 32.8 | 43.4 |
| **Reasoning** | MMLU Pro | 0 | acc | 68.9 | 73.4 | 74.3 | 80.5 |
| | GPQA Diamond | 0 | accuracy | 50.5 | 49.0 | 57.2 | 69.8 |
| **Multilingual** | MGSM | 0 | average/em | 91.1 | 91.6 | 90.6 | 92.3 |
| **Long Context** | MTOB (half book) | - | chrF | *Context < 128K* | *Context < 128K* | 42.2 / 36.6 | 54.0 / 46.4 |
| | MTOB (full book) | - | chrF | *Context < 128K* | *Context < 128K* | 39.7 / 36.3 | 50.8 / 46.7 |
---

> *\*Показники для MMMU Pro є середнім значенням стандартних та візуальних завдань.*
>
> Метрики дозволяють порівнювати:
> * Accuracy (%) – більший відсоток = кращі знання або логіка.
> * EM (Exact Match) – точне співпадіння з правильними відповідями.
> * pass@1 – ймовірність правильної першої відповіді у коді.
> * anls – оцінка точності розуміння документів/графіків.

### Підсумки та висновки по бенчмарках Llama 3.x та 4

1. Прогрес у reasoning та знаннях

       Llama 3.1 демонструє хороші результати в базових знаннях та математичних завданнях (MMLU, MATH), але версії 4 Scout та 4 Maverick значно покращили показники точності та EM – у MATH 4 Maverick досягає 61.2% точності, що на ~20% вище, ніж Llama 3.1 70B – це показує суттєвий приріст reasoning та математичних здібностей.

2. Кодинг та генерація коду

       В усіх бенчмарках з коду (MBPP, LiveCodeBench) помітне покращення з моделлю 4, особливо у Maverick, де pass@1 зростає до 77.6% для MBPP та 43.4% для LiveCodeBench – це означає, що останні версії Llama більш ефективні для автоматизованого програмування.

3. Мультимовність

       Показники TydiQA та MGSM демонструють стабільний рівень для всіх версій, з невеликим зростанням у Llama 4 Maverick (MGSM - 92.3%) – моделі зберігають сильні результати у розумінні тексту різними мовами.

4. Мультимодальні можливості (зображення, документи)

       Llama 3.x не підтримує задачі з графіками та документами, а Llama 4 відкриває можливості мультимодального розуміння – ChartQA та DocVQA демонструють високі результати (до 94.4% для DocVQA), що робить версію 4 корисною для аналізу візуальної інформації.

5. Довгі контексти

       Версії 4 значно покращили роботу з великими текстами (MTOB). Maverick краще справляється з повними книгами, досягаючи chrF ~50.8/46.7, що підтверджує покращення у long-context reasoning.

### Висновок
* Llama 3.x надійна для текстових завдань і коду, але обмежена мультимодально.
* Llama 4 Scout та Maverick – суттєво покращена версія (розширене reasoning, кращі результати у кодингу, підтримка мультимодальних задач та робота з довгим контекстом).
* **Maverick** – топова модель, оптимальна для комплексних завдань (від математичних задач до аналізу документів та зображень).

Отже, таблиці демонструють еволюцію моделей Meta: від сильної текстової бази (Llama 3) до універсальної мультимодальної моделі з високою точністю та розширеними можливостями (Llama 4 Maverick).


## 6. Інструменти та підходи Meta для розробки моделей Llama

### 6.1. Репозиторії, відкритий код та "Llama Stack"

Meta надає відкритий доступ до коду для inference, управління моделями, скриптів для запуску, fine-tuning і безпеки через відкриті репозиторії GitHub. 

* Репозиторій *meta‑llama/llama‑models* – центральний для foundation моделей LLaMA; містить «model cards», інструменти для завантаження, перевірки, опис моделей та CLI (команди llama-model list, download, describe, тощо) [[9]](#джерела). 
* Репозиторій *meta‑llama/llama* (хоч і відмічений як deprecated для старих версій) – дає початковий код для inference, інструкції для запуску моделей, ілюстрації роботи з LLaMA 2/3 [[10]](#джерела).
* Репозиторій *meta‑llama/codellama* – для моделей, спеціалізованих на коді (генерація, інфілінг, аналіз). Там є інструкції, файл LICENSE, model-card, опис моделей (7B, 13B, 34B…) і приклади запуску [[11]](#джерела).

Це базовий "робочий стек" – будь-хто зі знанням Python або PyTorch може завантажити, використати або fine-tune модель.

### 6.2. Доступність моделей, ліцензування і open-weights політика

* У llama-models описано, що моделі надаються з open access: weights доступні після прийняття ліцензії, для researcher-ів і комерційних організацій – за умовами Meta [[9]](#джерела). 
* Meta заявляє, що Llama – "доступна всім" – а це означає, що мета – це democratize AI – робить моделі доступними для дослідників, стартапів, індивідуальних розробників. 
* За офіційною інформацією Meta, сімейство моделей Llama досягло позначки 1 млрд завантажень [[12]](#джерела). Пізніше на конференції LlamaCon (29 квітня 2025) компанія повідомила про 1.2 млрд завантажень. Ці дані підтверджені прес-релізом Meta та висвітлені провідними виданнями [[13]](#джерела). 
Це означає, що Meta LLaMA – не просто "дослідницький проєкт", а екосистема з реальною індустріальною вагою.

Отже, Meta забезпечує відкритість weights + документації + прості інтерфейси для скачування/запуску – це важливо для реплікації, досліджень і інтеграцій.

### 6.3. Оптимізації та інструменти для inference/fine-tuning

З репозиторію *llama-models* видно, що Meta (або її open-source команда) передбачає підтримку quantization (наприклад, для Llama 4 можна вказати прапорець --quantization-mode (fp8_mixed або int4_mixed), що дозволяє зменшити пам’ять і прискорити inference) [[9]](#джерела).

    Це підтверджує, що Meta не лише публікує модель, а й дає інструменти для ресурсоефективного запуску на різному обладнанні.

Також архітектура Llama Stack передбачає модульність: 
* llama-toolchain
* llama-agentic-system
* PurpleLlama (безпекові шари, inference-оптимізації)
* llama-cookbook (скрипти, приклади, fine-tuning) [[10]](#джерела). 

Це означає, що Meta (спільнота) створили не просто модель, а повну екосистему, що дозволяє запускати, fine-tune, інтерфейсувати, експериментувати, враховувати безпеку і оптимізувати ресурси.

### 6.4. Спеціалізовані моделі: Code, Safety, Multimodal/Екосистема

* **Code** – через CodeLlama: Meta окремо релізувала версії моделі, натреновані або fine-tuned на код, оптимізовані для програмування, інфілінгу, генерації коду [[11]](#джерела). 

* **Безпека/контроль** – у відкритих репозиторіях не завжди є "офіційні white-paper", саме існування модулів як "PurpleLlama" і "llama-agentic-system", зі згадками про "safety shields/safety mitigations/inference-time mitigations" свідчить, що Meta орієнтована на відповідальне використання. 

* **Широка екосистема** – на GitHub видно багато пов'язаних репозиторіїв (llama-cookbook, synthetic-data-kit, prompt-ops, llama-verifications і т.д.) – це все інструменти, які спрощують fine-tuning, генерацію даних, інтеграцію, тестування [[10]](#джерела).

Таким чином, Meta розвиває не лише "моделі", а платформу та стек: foundation + code + інструменти + community.

### 6.5. Висновок

Meta не просто випускає окремі моделі – вона створює повну open-source екосистему, яка забезпечує:
* доступ до code + weights + документації;
* інструменти для inference, fine-tuning, agent-систем;
* оптимізації (quantization, modular stack) – робить моделі доступними навіть для невеликих команд або дослідників;
* підтримку спеціалізованих задач (код, безпека, multimodal, інструменти) через окремі моделі/репозиторії;
* широке поширення – Llama вже має сотні мільйонів завантажень.
* не всі архітектурні деталі (наприклад, точно які optimization-техніки застосовані, які dataset used, training recipes) – повністю публічні. Частина інформації закрита або недостатньо документована [[9]](#джерела).

Це означає, що Meta прагне не просто технологічного “рівня з OpenAI”, а – демократизації AI: щоб дослідники, стартапи та незалежні розробники могли мати доступ до потужних LLM з відкритим кодом і відповідною інфраструктурою.

## Джерела 
1. Reuters: Meta to spend up to $65 billion this year to power AI goals, Zuckerberg says. [Читати оригінал](https://www.reuters.com/technology/meta-invest-up-65-bln-capital-expenditure-this-year-2025-01-24/).

2. Reuters: Meta begins testing its first in-house AI training chip. [Читати оригінал](https://www.reuters.com/technology/artificial-intelligence/meta-begins-testing-its-first-in-house-ai-training-chip-2025-03-11/).

3. Llama 2.[Читати оригінал](https://github.com/meta-Llama/Llama/blob/main/MODEL_CARD.md).

4. Code Llama. [Читати оригінал](https://github.com/meta-llama/codellama/blob/main/README.md).

5. Llama 3. [Читати оригінал](https://github.com/meta-llama/llama-models/blob/main/models/llama3/MODEL_CARD.md).

6. Llama 3.1. [Читати оригінал](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md).

7. Llama 3.2: Model Cards And Prompt Formats. [Читати оригінал](https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_2/).

8. Llama 3.3 [Читати оригінал](https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md).

9. Git-repository "llama-models". [Посилання](https://github.com/meta-llama/llama-models).

10. Git-repository "llama". [Посилання](https://github.com/meta-llama/llama).

11. Git-repository "codellama". [Посилання](https://github.com/meta-llama/codellama).

12. Celebrating 1 Billion Downloads of Llama. [Читати оригінал](https://about.fb.com/news/2025/03/celebrating-1-billion-downloads-llama/amp).

13. Meta says its Llama AI models have been downloaded 1.2B times. [Читати оригінал](https://techcrunch.com/2025/04/29/meta-says-its-llama-ai-models-have-been-downloaded-1-2b-times/).
