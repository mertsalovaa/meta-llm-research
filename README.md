# Дослідження вендора великих мовних моделей: Meta

> **Виконала:**  
> студентка групи ІПЗ-51м  
> Мерцалова Ірина  

## Зміст
   [Словник термінів](#словник-термінів)
1. [Вступ: Meta як архітектор відкритої екосистеми ШІ](#1-вступ)
2. [Еволюція великих мовних моделей Meta](#2-еволюція-великих-мовних-моделей-meta)
    * [OPT](#21-opt)
    * [Galactica](#22-galactica)
    * [Llama 1](#23-Llama-1)
    * [Llama 2](#24-Llama-2) 
    * [Code Llama](#25-code-llama)
    * [Llama 3](#26-llama-3) 
    * [Llama 3.1](#27-llama-31) 
    * [Llama 3.2](#28-llama-32) 
    * [Llama 3.3](#29-llama-33) 
    * [Llama 4](#210-llama-4) 
    * [Llama Guard](#211-llama-guard) 
3. [Карта моделей](#3-карта-моделей)
4. [Ключові персони](#4-ключові-персони)
5. [Бенчмарки та порівняння](#5-бенчмарки-та-порівняння-ефективності)
6. [Інструменти та підходи Meta для розробки моделей Llama](#6-інструменти-та-підходи-meta-для-розробки-моделей-llama)
    * [Репозиторії, відкритий код та "Llama Stack"](#61-репозиторії-відкритий-код-та-llama-stack)
    * [Доступність моделей, ліцензування і open-weights політика](#62-доступність-моделей-ліцензування-і-open-weights-політика)
    * [Оптимізації та інструменти для inference/fine-tuning](#63-оптимізації-та-інструменти-для-inferencefine-tuning)
    * [Спеціалізовані моделі та розширення екосистеми](#64-спеціалізовані-моделі-та-розширення-екосистеми)
    * [Висновок](#65-висновок)
7. [Джерела](#джерела)
8. [Висновки та особисті враження від дослідження](#висновки-та-особисті-враження-від-дослідження)


## Словник термінів

Великі мовні моделі – моделі штучного інтелекту, натреновані на великих масивах текстових даних і здатні генерувати, узагальнювати та аналізувати інформацію.

Генеративний штучний інтелект – тип систем ШІ, призначених для створення нового контенту, зокрема тексту, зображень або коду.

Трансформер – архітектура нейронних мереж, що ґрунтується на механізмі самоуваги та використовується як основа для сучасних мовних моделей.

Інференс – процес отримання відповіді від натренованої моделі без додаткового навчання.

Тонке налаштування (*fine-tuning*) – додаткове навчання попередньо натренованої моделі на спеціалізованих даних для адаптації до конкретних завдань.

Навчання з підкріпленням на основі людського зворотного зв’язку (RLHF) – метод покращення якості відповідей моделі шляхом використання оцінок від людей.

Квантизація – метод оптимізації, за якого числову точність ваг моделі зменшують для зниження обчислювальних витрат.

Токен – мінімальна одиниця тексту, на яку токенізатор розбиває вхідні дані (символ, частина слова або слово).

Токенізація – процес поділу тексту на токени для подальшої обробки моделлю.

Токенізатор – алгоритм, що здійснює токенізацію (наприклад, SentencePiece, GPT-2 BPE, TikToken-based).

Контекстне вікно – максимальна кількість токенів, які модель може обробити за один запит.

Модель змішаних експертів (*Mixture-of-Experts*) – архітектура, у якій активуються різні підмодулі залежно від типу завдання, що підвищує продуктивність.

Мультимодальна модель – модель, здатна обробляти кілька типів даних (наприклад, текст і зображення).

Модель, налаштована інструкціями (*instruction-tuned model*) – модель, оптимізована для виконання інструктивних завдань і взаємодії у форматі діалогу.

Відкриті ваги (*open weights*) – спосіб поширення моделей, за якого їхні ваги доступні для досліджень, інференсу й удосконалення.

Карта моделі (*model card*) – документ із технічним описом моделі, що містить дані про навчання, обмеження, характеристики та рекомендації щодо використання.

Оптимізація інференсу – методи прискорення роботи моделі без зміни її вихідної архітектури.

Синтетичні дані – дані, згенеровані моделями або алгоритмами для збільшення навчального набору.

## 1. Вступ: Meta як архітектор відкритої екосистеми ШІ

Компанія Meta (раніше Facebook) бере участь у розвитку екосистеми великих мовних моделей, роблячи акцент на підході з відкритими вагами. На відміну від компаній, які використовують переважно закриті комерційні моделі, Meta публікує ваги деяких своїх моделей – це дозволяє дослідникам та розробникам виконувати локальний інференс і здійснювати подальше тонке налаштування.

Meta застосовує підхід Open Weights означає, що компанія віддає всі знання, які вона отримала за роки навчання своїх моделей, у відкритий доступ. Саме це дозволяє будь-якому розробнику завантажити модель собі на комп'ютер, користуватися нею без інтернету і, що найважливіше, донавчати її під свої власні, специфічні завдання.

*Ця стратегія має дві практичні цілі:*
1.  **Стандартизація індустрії –** поширення моделей Llama сприяє їх використанню розробниками у різних прикладних сценаріях, що дозволяє індустрії створювати нові інструменти та програми саме під Meta, закріплюючи за ними лідерство на ринку.
2.  **Краудсорсинг інновацій –** відкриваючи доступ до моделей, компанія залучає світову спільноту до роботи. Відкритість моделей дає змогу зовнішнім дослідникам пропонувати оптимізації та виявляти недоліки.

Варто також зазначити масштаб інвестицій в інфраструктуру. За даними Reuters, Meta прогнозує капітальні витрати на рівні до $65 млрд у 2025 році [[1]](#джерела). Як зазначив CEO Марк Цукерберг, ці кошти спрямовані на розширення AI-інфраструктури, щоб посилити позиції компанії у гонці проти OpenAI та Google.

Окрім масової закупівлі чіпів NVIDIA, Meta робить важливий стратегічний крок – компанія почала тестування власних процесорів (in-house chips) для тренування ШІ [[2]](#джерела). Це дозволить зменшити залежність від зовнішніх постачальників (зокрема NVIDIA) та отримати більш енергоефективні рішення для специфічних завдань.

Таким чином, Meta розвиває інструменти та інфраструктуру, пов’язані з моделями Llama, які робить їх придатними для широкого кола застосувань, на якій будується значна частина сучасної open-source AI економіки.

## 2. Еволюція великих мовних моделей Meta

У 2021–2022 роках провідні компанії (OpenAI, Google та інші лідери технологій) активно розробляли великі мовні моделі, але більшість з них залишалися закритими та недоступними для розробників. Саме тому, Meta застосувала стратегію відкритих ваг, яка відрізняється від підходів інших компаній – це дає масштабну відкритість, підтримку академічної спільноти та розвиток *open-source*-екосистеми.

Метою Meta було створення сімейства моделей, які можуть бути використані у наукових дослідженнях, промислових застосуваннях та освітніх проєктах без бар’єрів комерційної ліцензії – цей підхід став фундаментом усієї подальшої еволюції великих мовних моделей Meta.

### 2.1. OPT

**OPT (Open Pre-trained Transformer)** став першою масштабною відповіддю Meta на обмеженість доступу до великих мовних моделей. Модель мала варіації від 125M до 175B параметрів та відтворювала архітектуру GPT-3, але у відкритому форматі. OPT став відправною точкою, що відкрила Meta шлях до більш масштабних моделей.

Особливості:
* відтворення GPT-3-класи моделей у відкритому доступі
* публікація повного навчального коду та журналів тренування
* орієнтація на дослідників та університети

### 2.2. Galactica

**Galactica** була експериментальною моделлю, натренованою на наукових публікаціях, підручниках, хімічних та біологічних базах знань, та мала варіації від 125M до 120B параметрів.

Ключові характеристики:
* 120B параметрів у флагманській версії
* домен – наукова інформація
* генерація формул, цитувань, наукових пояснень

Попри високий рівень інноваційності, модель була знята з публічного доступу через ризик генерації неправдивого контенту. Проте досвід Galactica інтегрувався в наступні покоління великих мовних моделей.

### 2.3. Llama 1

У 2023 році Meta представила Llama (Large Language Model Meta AI) – нову архітектуру, оптимізовану для інференсу на менш потужному обладнанні. Llama 65B та Llama 33B навчені на 1,4 трильйона токенів. Найменша модель, Llama 7B, навчена на одному трильйоні токенів [[17]](#джерела).

Нововведення Llama 1:
* висока якість при менших параметрах
* відкритість моделі та можливість донавчання спільнотою
* зростання ролі open-source на ринку великих мовних моделей

Випуск Llama 1 привернув увагу спільноти до моделей Meta у форматі open-sourse моделей.

### 2.4. Llama 2

Llama 2 стала наступним етапом розвитку, у межах якого компанія вперше дозволила комерційне використання моделей. Вона складається з попередньо навчених та точно налаштованих генеративних текстових моделей масштабом від 7 до 70 мільярдів параметрів.

Точно налаштовані великі мовні моделі, які називаються Llama-2-Chat, оптимізовані для використання в діалогах. Моделі Llama-2-Chat перевершують моделі чату з відкритим кодом у більшості протестованих нами бенчмарків, а в наших людських оцінках корисності та безпеки вони знаходяться на одному рівні з деякими популярними моделями із закритим кодом, такими як ChatGPT та PaLM. [[3]](#джерела)

Покращення порівняно з Llama 1:
* збільшений обсяг тренувальних даних
* контрольоване тонке налаштування (SFT)
* навчання з підкріпленням та людським зворотним зв'язком (RLHF)
* значно краща якість у завданнях міркування
* спеціальна модель Code Llama для програмування

### 2.5. Code Llama

**Code Llama** – перша велика модель від Meta, створена для генерації та аналізу коду, що забезпечує конкурентні результати за низкою бенчмарків серед відкритих моделей, можливості заповнення, підтримку великих вхідних контекстів та можливість виконання інструкцій без повторного виконання для програмних завдань. 

Для навчання моделі використовувались спеціальні бібліотеки, тому тонке налаштування та навчання були виконані за допомогою дослідницького суперкластеру Meta.

Саме Code Llama активно використовується серед розробників у 2023-2024.

Переваги:
* підтримка понад 20 мов програмування
* здатність розуміти великі фрагменти коду, рефакторинг, коментування.
* варіації Code Llama випускається в чотирьох розмірах моделі та трьох варіантах: 
    * *Code Llama*: базова модель, яка розробленв для загального синтезу та розуміння коду
    * *Code Llama - Python* розроблена спеціально для роботи з мовою програмування Python
    * *Code Llama - Instruct* призначена для безпечнішого використання в програмах допомоги та генерації коду [[4]](#джерела).

### 2.6. Llama 3

Meta розробила та випустила сімейство великих мовних моделей Meta **Llama 3** – колекцію попередньо навчених та налаштованих на інструкції генеративних текстових моделей. Llama 3 випускається у двох розмірах – з параметрами 8B та 70B – у попередньо навчених та налаштованих за допомогою інструкцій варіантах, які оптимізовані для випадків використання діалогу та перевершують багато доступних моделей чату з відкритим кодом за загальними галузевими тестами. Крім того, розробляючи ці моделі, приділили значну увагу оптимізації корисності та безпеки.

Для попереднього навчання використовувались користувацькі навчальні бібліотеки, Meta's Research SuperCluster та виробничі кластери. тонке налаштування, анотування та оцінювання також виконувалися на сторонніх хмарних обчисленнях.

Llama 3 було попередньо навчено на понад 15 трильйонах токенів даних із загальнодоступних джерел. Дані для тонкого налаштування включають загальнодоступні набори даних інструкцій, а також понад 10 мільйонів прикладів з анотаціями від людини. Ні набори даних для попереднього навчання, ні набори даних для тонкого налаштування не містять метаданих користувачів.

Основні цінності Llama 3 – це відкритість, інклюзивність та готовність допомогти. Вона призначена для обслуговування всіх та для широкого кола випадків використання, щоб бути доступним для людей з різним походженням, досвідом та поглядами. Llama 3 звертається до користувачів та їхніх потреб такими, якими вони є, без зайвих осудів чи нормативності, водночас відображаючи розуміння того, що навіть контент, який може здаватися проблематичним в одних випадках, може служити цінним цілям в інших. Вона поважає гідність та автономію всіх користувачів, особливо з точки зору цінностей свободи думки та слова, які сприяють інноваціям та прогресу [[5]](#джерела).

LLaMA 3 отримала найширшу підтримку в екосистемі open-source.

### 2.7. Llama 3.1

Колекція багатомовних моделей **Llama 3.1** – це колекція попередньо навчених та налаштованих на інструкції генеративних моделей для введення чи виведення текст, яка призначена для комерційного та дослідницького використання. Моделі Llama 3.1 оптимізовані для використання багатомовних діалогів та перевершують багато доступних моделей з відкритим кодом та закритим чатом за загальними галузевими тестами.

Вона була попередньо навчена на ~15 трильйонах токенів даних із загальнодоступних джерел. Дані для тонкого налаштування включають загальнодоступні набори даних інструкцій, а також понад 25 мільйонів синтетично згенерованих прикладів.

Llama 3.1 підтримує 7 мов на додаток до англійської: французьку, німецьку, хінді, італійську, португальську, іспанську та тайську. Llama може виводити текст іншими мовами, окрім тих, що відповідають пороговим значенням продуктивності для безпеки та корисності. Не рекомендується розробникам використовувати модель для спілкування непідтримуваними мовами без впровадження тонкого налаштування та системних елементів керування [[6]](#джерела).

Флагманська модель демонструє результати, співставні з низкою закритих моделей у певних бенчмарках (включаючи GPT-4, GPT-4o та Claude 3.5 Sonnet).


### 2.8. Llama 3.2

**Llama 3.2** – це реліз, який має мультимодальні версії (Vision), текстові lightweight-версії (1B, 3B) та доступність через комерційні/хмарні сервіси (AWS). 

Для підтримки завдань розпізнавання зображень модель Llama 3.2-Vision використовує окремо навчений адаптер зору, який інтегрується з попередньо навченою мовною моделлю Llama 3.1. Адаптер складається з серії шарів перехресної уваги, які передають представлення кодера зображень до основної великої мовної моделі. Моделі з налаштованими інструкціями призначені для візуального розпізнавання, аналізу зображень, створення субтитрів та асистентоподібного чату із зображеннями.

Llama 3.2-Vision було попередньо навчено на 6B парах зображень та тексту. Дані налаштування інструкцій включають загальнодоступні набори даних інструкцій зору, а також понад 3M синтетично згенерованих прикладів [[7]](#джерела).

Ця модель перевершує багато доступних багатомодальних моделей з відкритим кодом та закритих багатомодальних моделей за загальними галузевими тестами.

### 2.9. Llama 3.3

**Модель Llama 3.3** – модель, яка працює лише з текстом, оптимізована для використання багатомовного діалогу та перевершує багато доступних моделей з відкритим кодом та закритим чатом за загальними галузевими тестами. Налаштовані версії використовують контрольоване тонке налаштування (SFT) та навчання з підкріпленням та людським зворотним зв'язком (RLHF) для узгодження з людськими вподобаннями щодо корисності та безпеки. Llama 3.3 призначена для комерційного та дослідницького використання кількома мовами, а також підтримує можливість використання результатів своїх актуальних моделей для покращення інших моделей, включаючи генерацію та дистиляцію синтетичних даних [[8]](#джерела). 

За даними Meta, модель демонструє покращені результати у завданнях логічного міркування та інструктивної взаємодії.

### 2.10. Llama 4

**Llama 4** – це колекція попередньо навчених та налаштованих на інструкції великих мовних моделей зі змішаними експертами, яка є наступним етапом розвитку моделей для екосистеми Llama. Запускається дві ефективні моделі серії Llama 4: *Llama 4 Scout* (модель з 17 мільярдами параметрів та 16 експертами) та *Llama 4 Maverick* (модель з 17 мільярдами параметрів та 128 експертами) – ці моделі оптимізовані для мультимодального розуміння, багатомовних завдань, кодування, виклику інструментів та підтримки агентних систем. Крайній термін знань для моделей – серпень 2024 року [[14]](#джерела).

Llama 4 Scout було попередньо навчено на ~40 трильйонах токенів, а Llama 4 Maverick – на ~22 трильйонах токенів мультимодальних даних з поєднання загальнодоступних, ліцензованих даних та інформації з продуктів і сервісів Meta – це включає публічно поширені публікації з Instagram та Facebook, а також взаємодію людей з Meta AI.

*Llama 4* – це більш керована модель, тобто відповіді можна легко адаптувати до конкретних потреб розробника. Ефективні системні підказки можуть значно підвищити продуктивність великих мовних моделей. Зокрема, провелось тестування, що використання системної підказки може бути ефективним у зменшенні кількості помилкових відмов та шаблонних або «повічливих» мовних моделей. Вони також можуть покращити розмовність та використання відповідного форматування.

### 2.11. Llama Guard

Meta створила окрему родину моделей Llama Guard, що відповідає за:
* модерацію контенту;
* виявлення небезпечних запитів;
* фільтрацію небажаних відповідей;
* захист від небезпечних інструкцій.

Вони використовуються у Facebook, Instagram, WhatsApp, а також інтегруються в open-source-екосистему. Були представлені Llama Guard 1, 2, 3 та 4.


## 3. Карта моделей

Нижче наведено зведену таблицю ключових характеристик моделей Meta AI. Ця "карта" демонструє прогрес у масштабуванні параметрів, кількості розмірів моделей та типів.

### Порівняльна таблиця сімейства великих мовних моделей Meta
---
|  **Модель** | **Розмір моделі** | **Тип** | **Дата випуску** |  **Довжина контексту** | **Токенізатор** | **Ліцензія** | **Model card** |
| :----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: |
| OPT | 125M, 350M, 1.3B, 2.7B, 6.7B, 13B, 30B, 66B, 175B | Текст | 01.05.2022 | 2K | GPT-2 BPE | Некомерційні дослідження | [Model Card](https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/README.md) |
| Galactica | 125M, 1.3B, 6.7B, 30B, 120B | Текст | 01.11.2022 | 2K | GPT-2 BPE | CC BY-NC 4.0 | [Model Card](https://github.com/paperswithcode/galai/blob/main/docs/model_card.md) |
| Llama (1) | 7B, 13B, 33B, 65B | Текст | 01.02.2023 | 2K | SentencePiece | Некомерційні дослідження | [Model Card](https://github.com/meta-llama/llama/blob/llama_v1/MODEL_CARD.md) |
| Llama 2 | 7B, 13B, 70B | Текст | 18.07.2023 |  4K | Sentencepiece | Llama 2 License (permissive) | [Model Card](https://github.com/meta-Llama/Llama/blob/main/MODEL_CARD.md) | 
| Llama 3 | 8B, 70B | Текст | 18.04.2024 |  8K | TikToken-based | Meta Llama 3 Community License | [Model Card](https://github.com/meta-llama/llama-models/blob/main/models/llama3/MODEL_CARD.md) |
| Llama 3.1 | 8B, 70B, 405В | Багатомовний текст | 23.07.2024 |  128K | TikToken-based | Meta Llama 3.1 Community License | [Model Card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md) |  
| Llama 3.2 | 1B, 3B | Багатомовний текст | 25.09.2024 |  128K | TikToken-based | Meta Llama 3.2 Community License | [Model Card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD.md) |
| Llama 3.2-Vision | 11B, 90B | Багатомовний текст + зображення | 25.09.2024 |  128K | TikToken-based | Meta Llama 3.2 Community License | [Model Card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/MODEL_CARD_VISION.md) |
| Llama 3.3 | 70B | Багатомовний текст | 06.12.2024 |  128K | TikToken-based | Meta Llama 3.3 Community License | [Model Card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md) |
| Llama 4 | Llama 4 Scout (17B) | Багатомовний текст + зображення | 05.04.2025 |  10М | TikToken-based | Meta Llama 4 Community License | [Model Card](https://github.com/meta-llama/llama-models/blob/main/models/llama4/MODEL_CARD.md) | 
| | Llama 4 Maverick (17B) | Багатомовний текст + зображення | 05.04.2025 |  1М | TikToken-based | Meta Llama 4 Community License | [Model Card](https://github.com/meta-llama/llama-models/blob/main/models/llama4/MODEL_CARD.md) | 
---


## 4. Ключові персони

Розвиток екосистеми Llama став можливим завдяки дослідницькому підрозділу **FAIR (Fundamental AI Research)**, яка відіграє важливу роль у розвитку моделей Llama та поєднує академічну свободу з ресурсами техногіганта. Успіх проєкту залежить від конкретних лідерів та інженерів.

---
| **Ім'я** | **Позиція** | **Роль** | **Вплив** | 
| :----: | :----: | :----: | :----: | 
| **Ян ЛеКун (Yann LeCun)** | Chief AI Scientist | Очолює напрям досліджень штучного інтелекту в Meta, лауреат премії Тюрінга (аналог Нобелівської премії в IT) | Саме ЛеКун є головним захисником стратегії **Open Weights**. Він переконаний, що закритий ШІ (як у OpenAI) небезпечний, оскільки концентрує владу в одних руках. Його філософія – "ШІ має бути відкритою науковою дисципліною" |
| **Марк Цукерберг (Mark Zuckerberg)** | CEO Meta | Замовник та спонсор | Прийняв стратегічне рішення переорієнтувати компанію на AI First та виділив бюджет на закупівлю 350,000+ чіпів NVIDIA H100. Його підтримка open-source підходу дозволила зробити Llama безкоштовним стандартом індустрії |
| **Жоель Піно (Joelle Pineau)** | VP of AI Research | Керівниця лабораторій FAIR | Відповідає за баланс між фундаментальною наукою та створенням реальних продуктів. Під її керівництвом дослідницькі проєкти перетворилися на продуктову лінійку Llama |
| **Ахмад Аль-Дахле (Ahmad Al-Dahle)** | VP of Generative AI | Керівник продуктового підрозділу GenAI | Саме він відповідає за перетворення наукових досліджень FAIR на реальні продукти. Його команда займалася тренуванням та випуском Llama 3, фокусуючись на тому, щоб модель була корисною для кінцевого користувача (coding, reasoning) |
| **Уго Туврон (Hugo Touvron)** | Lead Author |  | Його ім'я стоїть першим у наукових статтях про Llama 1 та Llama 2. Він керував технічною розробкою архітектури та процесом тренування моделей |
| **Томас Сіалом (Thomas Scialom)** | Керівник напрямку Fine-tuning & RLHF |  | Саме він відповідав за те, щоб "сира" модель навчилася вести діалог, бути ввічливою та безпечною (Llama 2 Chat). Без його роботи модель була б просто генератором тексту, а не помічником |
---

### Кадрові переходи та "Феномен Mistral"

Meta стала "кузнею кадрів" для всієї індустрії. Найяскравішим прикладом є відтік талантів, що призвів до появи головного європейського конкурента – **Mistral AI**.

**Гійом Лампль (Guillaume Lample)** та **Тімоті Лакруа (Timothée Lacroix)** – це провідні дослідники Meta AI, які були співавторами статті про Llama 1. Вони у 2023 році покинули Meta і заснували французький стартап **Mistral AI**. Використовуючи досвід, отриманий у Meta, вони створили моделі (Mistral 7B, Mixtral 8x7B), які певний час перевершували Llama за ефективністю.

Цей "витік мізків" демонструє, що Meta збирає найсильніших інженерів світу, але іноді втрачає їх через бюрократію або бажання дослідників створювати власні продукти.

## 5. Бенчмарки та порівняння ефективності

### 1: Pre-trained Models (Базові моделі) [[15]](#джерела)

---
| Бенчмарк | # Shots | Метрика      |    Llama 3.1 70B   |   Llama 3.1 405B   | Llama 4 Scout | Llama 4 Maverick |
| :------- | :-----: | :-----------: | :----------------: | :----------------: | :-----------: | :--------------: |
| MMLU     |    5    | acc_char     |        79.3        |        85.2        |      79.6     |       85.5       |
| MMLU-Pro |    5    | macro_avg/em |        53.8        |        61.6        |      58.2     |       62.9       |
| MATH     |    4    | em_maj1@1    |        41.6        |        53.5        |      50.3     |       61.2       |
| MBPP     |    3    | pass@1       |        66.4        |        74.4        |      67.8     |       77.6       |
| TydiQA   |    1    | average/f1   |        29.9        |        34.3        |      31.5     |       31.7       |
| ChartQA  |    0    | accuracy     | *Не підтримується* | *Не підтримується* |      83.4     |       85.3       |
| DocVQA   |    0    | anls         | *Не підтримується* | *Не підтримується* |      89.4     |       91.6       |

---

### 2: Instruction Tuned Models (Чат-версії) [[16]](#джерела)

---
| Бенчмарк         | # Shots | Метрика |    Llama 3.3 70B   |   Llama 3.1 405B   | Llama 4 Scout | Llama 4 Maverick |
| :--------------- | :-------: | :---------: | :----------------: | :----------------: | :-----------: | :--------------: |
| MMMU             |    0    | accuracy   | *Не підтримується* | *Не підтримується* |      69.4     |       73.4       |
| MMMU Pro*        |    0    | accuracy   | *Не підтримується* | *Не підтримується* |      52.2     |       59.6       |
| MathVista        |    0    | accuracy   | *Не підтримується* | *Не підтримується* |      70.7     |       73.7       |
| ChartQA          |    0    | accuracy   | *Не підтримується* | *Не підтримується* |      88.8     |       90.0       |
| DocVQA (test)    |    0    | anls       | *Не підтримується* | *Не підтримується* |      94.4     |       94.4       |
| LiveCodeBench    |    0    | pass@1     |        33.3        |        27.7        |      32.8     |       43.4       |
| MMLU Pro         |    0    | acc        |        68.9        |        73.4        |      74.3     |       80.5       |
| GPQA Diamond     |    0    | accuracy   |        50.5        |        49.0        |      57.2     |       69.8       |
| MGSM             |    0    | average/em |        91.1        |        91.6        |      90.6     |       92.3       |
| MTOB (half book) |    -    | chrF       |  *Context < 128K*  |  *Context < 128K*  |  42.2 / 36.6  |    54.0 / 46.4   |
| MTOB (full book) |    -    | chrF       |  *Context < 128K*  |  *Context < 128K*  |  39.7 / 36.3  |    50.8 / 46.7   |

---

> *\*Показники для MMMU Pro є середнім значенням стандартних та візуальних завдань.*
>
> Метрики дозволяють порівнювати:
> * Accuracy (%) – більший відсоток = кращі знання або логіка.
> * EM (Exact Match) – тонке співпадіння з правильними відповідями.
> * pass@1 – ймовірність правильної першої відповіді у коді.
> * anls – оцінка точності розуміння документів/графіків.

### Підсумки та висновки по бенчмарках Llama 3.x та 4

1. Прогрес у reasoning та знаннях

   > Llama 3.1 демонструє хороші результати в базових знаннях та математичних завданнях (MMLU, MATH), але версії 4 Scout та 4 Maverick значно покращили показники точності та EM – у MATH 4 Maverick досягає 61.2% точності, що на ~20% вище, ніж Llama 3.1 70B – це показує суттєвий приріст reasoning та математичних здібностей.

2. Кодинг та генерація коду

   > В усіх бенчмарках з коду (MBPP, LiveCodeBench) помітне покращення з моделлю 4, особливо у Maverick, де pass@1 зростає до 77.6% для MBPP та 43.4% для LiveCodeBench – це означає, що останні версії Llama більш ефективні для автоматизованого програмування.

3. Мультимовність

   > Показники TydiQA та MGSM демонструють стабільний рівень для всіх версій, з невеликим зростанням у Llama 4 Maverick (MGSM - 92.3%) – моделі зберігають сильні результати у розумінні тексту різними мовами.

4. Мультимодальні можливості (зображення, документи)
   > Llama 3.x не підтримує задачі з графіками та документами, а Llama 4 відкриває можливості мультимодального розуміння – ChartQA та DocVQA демонструють високі результати (до 94.4% для DocVQA), що робить версію 4 корисною для аналізу візуальної інформації.

5. Довгі контексти

   > Версії 4 значно покращили роботу з великими текстами (MTOB). Maverick краще справляється з повними книгами, досягаючи chrF ~50.8/46.7, що підтверджує покращення у long-context reasoning.

### Висновок

Еволюція моделей Meta демонструє поступовий перехід від високоякісних текстових систем до універсальних мультимодальних архітектур. Моделі лінійки Llama 3.x забезпечують надійну роботу з текстом і програмним кодом, однак їхні мультимодальні можливості залишаються обмеженими. Наступне покоління *Llama 4 Scout* та *Llama 4 Maverick* – характеризується суттєвим удосконаленням: вони показують підвищену здатність до міркування, демонструють кращі результати у задачах кодування, підтримують мультимодальні сценарії та ефективно працюють із довгими контекстами. Серед них Maverick позиціонується як флагманська модель, оптимізована для вирішення комплексних задач, які охоплюють математичні обчислення, аналіз великих масивів текстових даних, інтерпретацію документів та обробку зображень. 

У підсумку, перехід від Llama 3 до Llama 4 свідчить про масштабне розширення функціональності моделей Meta – від сильних текстових систем до мультимодальних інтелектуальних рішень.

## 6. Інструменти та підходи Meta для розробки моделей Llama

### 6.1. Репозиторії, відкритий код та "Llama Stack"

Meta забезпечує відкритий доступ до значної частини інфраструктури Llama через GitHub, де публікує код для inference, управління моделями, системних скриптів, fine-tuning та модулів безпеки. 

Центральним елементом є репозиторій *meta-llama/llama-models*, який містить model cards, інструменти для завантаження та перевірки моделей, CLI-команди (зокрема *llama-model* list, download, describe) та всю офіційно задокументовану технічну частину [[9]](#джерела). Поруч із ним історично існував репозиторій *meta-llama/llama*, який сьогодні позначений як застарілий, але продовжує містити базовий код для inference попередніх поколінь моделей, інструкції з розгортання та практичні приклади використання [[10]](#джерела).

Спеціалізований розвиток представлено окремим репозиторієм *meta-llama/codellama*, де зібрані моделі, оптимізовані для роботи з програмним кодом: генерації, інфілінгу та аналізу [[11]](#джерела). 

Усі ці репозиторії формують основу того, що Meta називає *Llama Stack* – базовий відкритий технологічний стек, який дозволяє будь-якому розробнику з базовими знаннями Python чи PyTorch завантажувати, налаштовувати або донавчати моделі.

### 6.2. Доступність моделей, ліцензування і open-weights політика

Усі ключові моделі, представлені в *llama-models*, постачаються за принципом open access. Meta відкриває ваги моделей після прийняття користувачем ліцензії, надаючи право використовувати їх як у дослідницьких, так і в комерційних цілях відповідно до встановлених умов [[9]](#джерела). У своїх офіційних документах компанія наголошує, що Llama створюється з метою "демократизації AI", а отже – має бути доступною широкому колу користувачів: від академічних лабораторій до незалежних розробників та стартапів.

Ефективність такої моделі поширення підтверджується статистикою Meta: у 2024 році кількість завантажень сімейства Llama перевищила 1 млрд [[12]](#джерела), а на конференції LlamaCon (квітень 2025 року) компанія повідомила про досягнення позначки 1.2 млрд завантажень – це  було також висвітлено в профільних медіа [[13]](#джерела).

Таким чином, Meta забезпечує відкритий доступ не лише до самих ваг, але й до всієї підтримувальної інфраструктури – документації, інструментів, інтерфейсів для завантаження та запуску, що робить можливими реплікації, дослідження і швидку інтеграцію моделей.

### 6.3. Оптимізації та інструменти для inference та fine-tuning

У репозиторії *llama-models* містяться офіційні інструменти оптимізації, зокрема підтримка різних режимів quantization. Наприклад, для Llama 4 передбачені параметри *--quantization-mode* fp8_mixed або int4_mixed, які зменшують обсяг необхідної пам’яті та покращують продуктивність inference – це демонструє, що Meta не лише публікує модель, але й приділяє значну увагу доступності її використання на різних апаратних конфігураціях [[9]](#джерела).

Архітектура Llama Stack має модульну структуру, що включає *llama-toolchain*, *llama-agentic-system*, безпекову ініціативу *PurpleLlama* та репозиторій *llama-cookbook* із прикладами та скриптами для *fine-tuning* – завдяки цьому Llama перетворюється на платформу, де розробник може повністю контролювати процес – від тренування до оптимізації та побудови агентних систем [[10]](#джерела).

### 6.4. Спеціалізовані моделі та розширення екосистеми

Окремий напрям розвитку становлять спеціалізовані моделі. *Code Llama* орієнтована на задачі програмної інженерії, демонструючи кращі можливості генерації та редагування коду [[11]](#джерела). Такі модулі як *PurpleLlama* формують шар безпеки для інференсу, включаючи механізми фільтрації, пом’якшення ризиків та контролю поведінки моделі. 

Крім того, існує широкий спектр підтримувальних інструментів – *llama-cookbook*, *synthetic-data-kit*, *llama-verifications*, *prompt-ops* та багато інших, що забезпечують зручність навчання, тестування, збору синтетичних даних та створення пайплайнів [[10]](#джерела).

Загалом екосистема Meta – це більше, ніж набір моделей – це цілісний технологічний стек, який об’єднує foundation-моделі, інструменти для коду, безпекові рішення, мультимодальні можливості та активну open-source спільноту.

### 6.5. Висновок

Отже, можна зазначити, що Meta не обмежується випуском окремих моделей, натомість формує повноцінну інфраструктуру для відкритого використання сучасних великих мовних моделей. Компанія забезпечує доступ до ваг, вихідного коду та детальної документації, розробляє інструменти для inference, fine-tuning та побудови агентних систем, а також впроваджує оптимізаційні технології, які дозволяють ефективно використовувати моделі навіть у невеликих командах [[9]](#джерела). Екосистема включає спеціалізовані рішення для коду, безпеки та мультимодальних завдань, а показники поширення підтверджують її реальну індустріальну значущість. 

Хоча Meta не розкриває абсолютно всі технічні деталі (зокрема повні training-pipelines чи dataset-recipes), її стратегія прозорості та відкритих ваг суттєво сприяє демократизації ШІ та розширює можливості для дослідників і розробників по всьому світу.

## Висновки та особисті враження від дослідження

Під час роботи над цим дослідженням моє сприйняття компанії Meta змінилося. Раніше я асоціювала їх переважно із соціальними мережами, проте під час підготовки матеріалу я змогла глибше зрозуміти не лише архітектуру та еволюцію сімейства моделей Llama, але загальну логіку розвитку відкритих великих мовних моделей, яку сформувала Meta. 

Аналізуючи моделі Llama 1–4, я побачила, як змінювалась їхня орієнтація: від базових дослідницьких систем – до сучасних, оптимізованих для реальних продуктів моделей, адаптованих під різні сценарії використання.

Під час опрацювання документації та моделей стало зрозуміло, що архітектура Llama є прикладом раціонального підходу до масштабування моделей. Meta не переслідує лише гонитву за кількістю параметрів – навпаки, увага приділена ефективності, оптимізації обчислень і можливості запускати моделі навіть у порівняно обмежених середовищах. Такий підхід дозволяє уникати надлишкової складності та дає змогу фокусуватися на якості навчання, а не на сліпому збільшенні параметрів.

Окремим аспектом, який я зрозуміла під час роботи, є те, що Llama значною мірою вплинула на баланс між відкритими і закритими моделями. Відкритість Llama не тільки спрощує використання моделей у навчальних чи дослідницьких цілях, але дозволяє спільноті швидко експериментувати з оптимізаціями, покращеннями та адаптаціями. Саме це формує довгострокову цінність таких моделей і робить їх основою для інновацій. Також варто зазначити, що попри відкритість ваг, ліцензійна політика Meta все ще містить певні обмеження для комерційного використання та не є Open Source у класичному розумінні цього терміна, що іноді створює юридичну невизначеність для стартапів.

У підсумку, робота над матеріалом показала, що Meta Llama – це приклад еволюції не лише моделей, але підходів до створення відкритих великих мовних моделей. Водночас потенціал для розвитку лишається значним: від розширення функціональності та контекстного вікна до глибшої документації та більшої мультимодальності – це демонструє, як відкриті моделі можуть бути одночасно ефективними, доступними та такими, які постійно розвиваються разом зі спільнотою.


## Джерела 
1. Reuters: Meta to spend up to $65 billion this year to power AI goals, Zuckerberg says. [Читати оригінал](https://www.reuters.com/technology/meta-invest-up-65-bln-capital-expenditure-this-year-2025-01-24/).

2. Reuters: Meta begins testing its first in-house AI training chip. [Читати оригінал](https://www.reuters.com/technology/artificial-intelligence/meta-begins-testing-its-first-in-house-ai-training-chip-2025-03-11/).

3. Llama 2.[Читати оригінал](https://github.com/meta-Llama/Llama/blob/main/MODEL_CARD.md).

4. Code Llama. [Читати оригінал](https://github.com/meta-llama/codellama/blob/main/README.md).

5. Llama 3. [Читати оригінал](https://github.com/meta-llama/llama-models/blob/main/models/llama3/MODEL_CARD.md).

6. Llama 3.1. [Читати оригінал](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md).

7. Llama 3.2: Model Cards And Prompt Formats. [Читати оригінал](https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_2/).

8. Llama 3.3 [Читати оригінал](https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md).

9. Git-repository "llama-models". [Посилання](https://github.com/meta-llama/llama-models).

10. Git-repository "llama". [Посилання](https://github.com/meta-llama/llama).

11. Git-repository "codellama". [Посилання](https://github.com/meta-llama/codellama).

12. Celebrating 1 Billion Downloads of Llama. [Читати оригінал](https://about.fb.com/news/2025/03/celebrating-1-billion-downloads-llama/amp).

13. Meta says its Llama AI models have been downloaded 1.2B times. [Читати оригінал](https://techcrunch.com/2025/04/29/meta-says-its-llama-ai-models-have-been-downloaded-1-2b-times/).

14. Llama 4. [Читати оригінал](https://github.com/meta-llama/llama-models/blob/main/models/llama4/MODEL_CARD.md).

15. Benchmarks: Pre-trained models. [Читати оригінал](https://github.com/meta-llama/llama-models/blob/main/models/llama4/MODEL_CARD.md#pre-trained-models).

16. Benchmarks: Instruction tuned models. [Читати оригінал](https://github.com/meta-llama/llama-models/blob/main/models/llama4/MODEL_CARD.md#instruction-tuned-models).

17. Llama (1). [Читати оригінал](https://github.com/meta-llama/llama/blob/llama_v1/MODEL_CARD.md)
